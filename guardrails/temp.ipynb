{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard.from_rail(\"temp.rail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctors_notes = \"\"\"49 y/o Male with chronic macular rash to face & hair, worse in beard, eyebrows & nares. Itchy, flaky, slightly scaly. Moderate response to OTC steroid cream\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m raw_llm_response, validated_output \u001b[39m=\u001b[39m guard(\n\u001b[1;32m      2\u001b[0m     openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate,\n\u001b[1;32m      3\u001b[0m     prompt_params\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mdoctors_notes\u001b[39;49m\u001b[39m\"\u001b[39;49m: doctors_notes},\n\u001b[1;32m      4\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/guardrails/guardrails/guard.py:165\u001b[0m, in \u001b[0;36mGuard.__call__\u001b[0;34m(self, llm_api, prompt_params, num_reasks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39m# TODO(shreya): should we overwrite self.instructions for this run?\u001b[39;00m\n\u001b[1;32m    156\u001b[0m runner \u001b[39m=\u001b[39m Runner(\n\u001b[1;32m    157\u001b[0m     instructions\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39minstructions\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstructions),\n\u001b[1;32m    158\u001b[0m     prompt\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     reask_prompt\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreask_prompt,\n\u001b[1;32m    164\u001b[0m )\n\u001b[0;32m--> 165\u001b[0m guard_history \u001b[39m=\u001b[39m runner(prompt_params\u001b[39m=\u001b[39;49mprompt_params)\n\u001b[1;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mguard_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mguard_state\u001b[39m.\u001b[39mpush(guard_history)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m guard_history\u001b[39m.\u001b[39moutput, guard_history\u001b[39m.\u001b[39mvalidated_output\n",
      "File \u001b[0;32m~/guardrails/guardrails/run.py:95\u001b[0m, in \u001b[0;36mRunner.__call__\u001b[0;34m(self, prompt_params)\u001b[0m\n\u001b[1;32m     87\u001b[0m instructions, prompt, input_schema, output_schema \u001b[39m=\u001b[39m (\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstructions,\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt,\n\u001b[1;32m     90\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_schema,\n\u001b[1;32m     91\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_schema,\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_reasks \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     94\u001b[0m     \u001b[39m# Run a single step.\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     validated_output, reasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     96\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m     97\u001b[0m         api\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi,\n\u001b[1;32m     98\u001b[0m         instructions\u001b[39m=\u001b[39;49minstructions,\n\u001b[1;32m     99\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m    100\u001b[0m         prompt_params\u001b[39m=\u001b[39;49mprompt_params,\n\u001b[1;32m    101\u001b[0m         input_schema\u001b[39m=\u001b[39;49minput_schema,\n\u001b[1;32m    102\u001b[0m         output_schema\u001b[39m=\u001b[39;49moutput_schema,\n\u001b[1;32m    103\u001b[0m         output\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput \u001b[39mif\u001b[39;49;00m index \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     \u001b[39m# Loop again?\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_loop(index, reasks):\n",
      "File \u001b[0;32m~/guardrails/guardrails/run.py:162\u001b[0m, in \u001b[0;36mRunner.step\u001b[0;34m(self, index, api, instructions, prompt, prompt_params, input_schema, output_schema, output)\u001b[0m\n\u001b[1;32m    159\u001b[0m     validated_output \u001b[39m=\u001b[39m sub_reasks_with_fixed_values(validated_output)\n\u001b[1;32m    161\u001b[0m \u001b[39m# Log: step information.\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog(\n\u001b[1;32m    163\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m    164\u001b[0m     instructions\u001b[39m=\u001b[39;49minstructions,\n\u001b[1;32m    165\u001b[0m     output\u001b[39m=\u001b[39;49moutput,\n\u001b[1;32m    166\u001b[0m     output_as_dict\u001b[39m=\u001b[39;49moutput_as_dict,\n\u001b[1;32m    167\u001b[0m     validated_output\u001b[39m=\u001b[39;49mvalidated_output,\n\u001b[1;32m    168\u001b[0m     reasks\u001b[39m=\u001b[39;49mreasks,\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[39mreturn\u001b[39;00m validated_output, reasks\n",
      "File \u001b[0;32m~/guardrails/guardrails/run.py:304\u001b[0m, in \u001b[0;36mRunner.log\u001b[0;34m(self, prompt, instructions, output, output_as_dict, validated_output, reasks)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog\u001b[39m(\n\u001b[1;32m    295\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    296\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m     reasks: \u001b[39mlist\u001b[39m,\n\u001b[1;32m    302\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Log the step.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mguard_history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mguard_history\u001b[39m.\u001b[39;49mpush(\n\u001b[1;32m    305\u001b[0m         GuardLogs(\n\u001b[1;32m    306\u001b[0m             prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m    307\u001b[0m             instructions\u001b[39m=\u001b[39;49minstructions,\n\u001b[1;32m    308\u001b[0m             output\u001b[39m=\u001b[39;49moutput,\n\u001b[1;32m    309\u001b[0m             output_as_dict\u001b[39m=\u001b[39;49moutput_as_dict,\n\u001b[1;32m    310\u001b[0m             validated_output\u001b[39m=\u001b[39;49mvalidated_output,\n\u001b[1;32m    311\u001b[0m             reasks\u001b[39m=\u001b[39;49mreasks,\n\u001b[1;32m    312\u001b[0m         )\n\u001b[1;32m    313\u001b[0m     )\n",
      "File \u001b[0;32m~/guardrails/guardrails/utils/logs_utils.py:70\u001b[0m, in \u001b[0;36mGuardHistory.push\u001b[0;34m(self, guard_log)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     69\u001b[0m     last_log \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 70\u001b[0m     guard_log\u001b[39m.\u001b[39mvalidated_output \u001b[39m=\u001b[39m merge_reask_output(last_log, guard_log)\n\u001b[1;32m     72\u001b[0m \u001b[39mreturn\u001b[39;00m GuardHistory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m+\u001b[39m [guard_log])\n",
      "File \u001b[0;32m~/guardrails/guardrails/utils/logs_utils.py:181\u001b[0m, in \u001b[0;36mmerge_reask_output\u001b[0;34m(prev_logs, current_logs)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m                 update_reasked_elements(\n\u001b[1;32m    178\u001b[0m                     pruned_reask_json[i], reask_response_dict[i]\n\u001b[1;32m    179\u001b[0m                 )\n\u001b[0;32m--> 181\u001b[0m update_reasked_elements(pruned_reask_json, reask_response)\n\u001b[1;32m    183\u001b[0m \u001b[39mreturn\u001b[39;00m merged_json\n",
      "File \u001b[0;32m~/guardrails/guardrails/utils/logs_utils.py:169\u001b[0m, in \u001b[0;36mmerge_reask_output.<locals>.update_reasked_elements\u001b[0;34m(pruned_reask_json, reask_response_dict)\u001b[0m\n\u001b[1;32m    166\u001b[0m             update_response_by_path(merged_json, value\u001b[39m.\u001b[39mpath, corrected_value)\n\u001b[1;32m    167\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m             update_reasked_elements(\n\u001b[0;32m--> 169\u001b[0m                 pruned_reask_json[key], reask_response_dict[key]\n\u001b[1;32m    170\u001b[0m             )\n\u001b[1;32m    171\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pruned_reask_json, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m i, item \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pruned_reask_json):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "raw_llm_response, validated_output = guard(\n",
    "    openai.ChatCompletion.create,\n",
    "    prompt_params={\"doctors_notes\": doctors_notes},\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guard.guard_state.all_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gd-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
